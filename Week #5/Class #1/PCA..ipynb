{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "4f473631-69a1-4248-bc72-c6f6b13f875f",
            "metadata": {},
            "source": [
                "# Comprehensive Tutorial on Principal Component Analysis (PCA)\n",
                "\n",
                "In this tutorial, we will cover:\n",
                "\n",
                "1. **Understanding PCA:** An explanation of what PCA is, why it is used, and how it works.\n",
                "2. **PCA for Regression:** Using PCA on the California Housing dataset to reduce dimensionality and build a regression model.\n",
                "3. **Other Applications of PCA:** Additional examples, such as using PCA for clustering/classification (with the Iris dataset).\n",
                "\n",
                "Let's start with an in-depth explanation of PCA."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "81c1a68b-5e21-4c7f-8f1d-4830f01630ee",
            "metadata": {},
            "source": [
                "## What is PCA?\n",
                "\n",
                "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms a dataset with many features into a new coordinate system with fewer dimensions (called principal components) while retaining most of the original variance.\n",
                "\n",
                "### Key Points:\n",
                "\n",
                "- **Dimensionality Reduction:** Simplifies complex data by reducing the number of variables, which can help in visualization and model performance.\n",
                "- **Variance Retention:** The first principal component captures the highest variance; subsequent components capture the remaining variance under the constraint of orthogonality.\n",
                "- **De-correlation:** The resulting principal components are uncorrelated (orthogonal) linear combinations of the original variables.\n",
                "\n",
                "### Why Use PCA?\n",
                "\n",
                "1. **Reduce the Curse of Dimensionality:** Fewer dimensions can lead to better model generalization and reduced computational cost.\n",
                "2. **Visualization:** Reducing data to 2 or 3 dimensions helps in visualizing complex datasets.\n",
                "3. **Noise Reduction:** By discarding components with low variance, you remove noise and redundant features.\n",
                "4. **Improved Interpretability:** Simplified models that are easier to understand and analyze.\n",
                "\n",
                "### How Does PCA Work?\n",
                "\n",
                "1. **Standardization:** Scale the data so that each feature has zero mean and unit variance.\n",
                "2. **Covariance Matrix Calculation:** Compute the covariance matrix to understand how features vary together.\n",
                "3. **Eigen Decomposition:** Extract eigenvalues and eigenvectors from the covariance matrix. Eigenvectors determine the directions (principal components), and eigenvalues determine their magnitude (variance).\n",
                "4. **Sorting and Selection:** Sort the eigenvectors by their corresponding eigenvalues in descending order and select the top components.\n",
                "5. **Projection:** Transform the original data onto the new lower-dimensional subspace formed by the selected principal components.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c4b9b1a5-69e1-44c3-beb6-cd67917b9b8d",
            "metadata": {},
            "source": [
                "## Application 1: PCA for Regression\n",
                "\n",
                "We will use the California Housing dataset to demonstrate how PCA can be applied to reduce dimensionality before building a regression model. The steps include:\n",
                "\n",
                "1. Loading and inspecting the dataset.\n",
                "2. Standardizing the data.\n",
                "3. Applying PCA to reduce the dataset to 2 principal components.\n",
                "4. Visualizing the PCA-transformed data.\n",
                "5. Using the PCA components in a regression model to predict house prices.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da3e92c1-64e1-4a63-ace2-d6395d9bfc68",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from sklearn.datasets import fetch_california_housing\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "# Enable inline plotting\n",
                "%matplotlib inline\n",
                "\n",
                "# Set a plotting style\n",
                "plt.style.use('seaborn')\n",
                "\n",
                "# ------------------------------------------\n",
                "# Step 1: Load the California Housing Dataset\n",
                "# ------------------------------------------\n",
                "housing = fetch_california_housing()\n",
                "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
                "y = housing.target  # Median house value\n",
                "\n",
                "print(\"First 5 rows of the California Housing dataset:\")\n",
                "print(X.head())\n",
                "\n",
                "# ------------------------------------------\n",
                "# Step 2: Standardize the Data\n",
                "# ------------------------------------------\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "print(\"Shape of standardized data:\", X_scaled.shape)\n",
                "\n",
                "# ------------------------------------------\n",
                "# Step 3: Apply PCA\n",
                "# ------------------------------------------\n",
                "n_components = 2  # We choose 2 principal components\n",
                "pca = PCA(n_components=n_components)\n",
                "X_pca = pca.fit_transform(X_scaled)\n",
                "print(\"Shape of PCA-transformed data:\", X_pca.shape)\n",
                "\n",
                "# ------------------------------------------\n",
                "# Step 4: Examine the Explained Variance Ratio\n",
                "# ------------------------------------------\n",
                "explained_variance = pca.explained_variance_ratio_\n",
                "print(f\"\\nExplained Variance Ratio for {n_components} components:\")\n",
                "for i, ratio in enumerate(explained_variance, 1):\n",
                "    print(f\"Principal Component {i}: {ratio:.2%}\")\n",
                "\n",
                "# ------------------------------------------\n",
                "# Step 5: Visualize the PCA-Reduced Data\n",
                "# ------------------------------------------\n",
                "plt.figure(figsize=(10, 7))\n",
                "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.6)\n",
                "plt.xlabel(\"Principal Component 1\")\n",
                "plt.ylabel(\"Principal Component 2\")\n",
                "plt.title(\"PCA of California Housing Dataset\")\n",
                "cbar = plt.colorbar(scatter)\n",
                "cbar.set_label(\"Median House Value\")\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "# ------------------------------------------\n",
                "# Step 6: Use PCA Components in a Regression Model\n",
                "# ------------------------------------------\n",
                "# Split the PCA data into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Train a linear regression model\n",
                "regressor = LinearRegression()\n",
                "regressor.fit(X_train, y_train)\n",
                "\n",
                "# Predict on the test set\n",
                "y_pred = regressor.predict(X_test)\n",
                "\n",
                "# Evaluate the model using Mean Squared Error (MSE)\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "print(f\"\\nMean Squared Error on test set using PCA components: {mse:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e4d15f5f-40ea-4f06-94e7-cb3f0b4b7ad6",
            "metadata": {},
            "source": [
                "## Application 2: PCA for Clustering / Classification\n",
                "\n",
                "PCA can also be used to reduce dimensions for visualization or as a preprocessing step for clustering and classification tasks. In this example, we will use the Iris dataset to:\n",
                "\n",
                "1. Apply PCA to reduce the data to 2 dimensions.\n",
                "2. Visualize the data with the true species labels.\n",
                "\n",
                "This is useful for understanding how well the data separates into distinct classes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "03a60b90-31b3-4d43-b308-3d1afdb2ff57",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import additional library for the Iris dataset\n",
                "from sklearn.datasets import load_iris\n",
                "\n",
                "# Load the Iris dataset\n",
                "iris = load_iris()\n",
                "X_iris = iris.data\n",
                "y_iris = iris.target\n",
                "target_names = iris.target_names\n",
                "\n",
                "# Standardize the Iris data\n",
                "scaler_iris = StandardScaler()\n",
                "X_iris_scaled = scaler_iris.fit_transform(X_iris)\n",
                "\n",
                "# Apply PCA to reduce to 2 components\n",
                "pca_iris = PCA(n_components=2)\n",
                "X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n",
                "\n",
                "# Print the explained variance ratio\n",
                "print(\"Explained variance ratio for Iris PCA:\", pca_iris.explained_variance_ratio_)\n",
                "\n",
                "# Visualize the PCA-reduced Iris data\n",
                "plt.figure(figsize=(8,6))\n",
                "for target, color in zip(np.unique(y_iris), ['navy', 'turquoise', 'darkorange']):\n",
                "    plt.scatter(X_iris_pca[y_iris == target, 0],\n",
                "                X_iris_pca[y_iris == target, 1],\n",
                "                label=target_names[target],\n",
                "                color=color,\n",
                "                alpha=0.7)\n",
                "\n",
                "plt.xlabel('Principal Component 1')\n",
                "plt.ylabel('Principal Component 2')\n",
                "plt.title('PCA of Iris Dataset')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ae4efcdc-ef3d-4246-8c6e-08f7a2165de7",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "In this tutorial, we have learned:\n",
                "\n",
                "- **What PCA is and why it is important:**\n",
                "  - Dimensionality reduction, noise reduction, and improved computational efficiency.\n",
                "\n",
                "- **How PCA works:**\n",
                "  - Standardization, covariance matrix calculation, eigen decomposition, and projection.\n",
                "\n",
                "- **Application of PCA in Regression:**\n",
                "  - We applied PCA on the California Housing dataset, visualized the results, and built a regression model.\n",
                "\n",
                "- **Application of PCA in Clustering/Classification:**\n",
                "  - We reduced the dimensions of the Iris dataset and visualized the data to observe class separability.\n",
                "\n",
                "PCA is a powerful tool for simplifying complex datasets, enabling more efficient machine learning and better visualization. Feel free to experiment further with different numbers of components or apply PCA to other datasets.\n",
                "\n",
                "Happy Teaching and Coding!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}