Part I: Introduction and Core Concepts
1.1 Why Linux for Data Science?
Linux has long been the preferred operating system (OS) for scientific computing, server environments, and large-scale data processing systems. While data science can certainly be done on other operating systems (Windows, macOS), the open-source nature, robustness, and flexibility of Linux distributions offer several advantages:

Stability and Security: Linux is known for its reliability and security features. Frequent security patches and a strong development community help maintain a secure environment for data-intensive tasks.
Community and Open-Source Ecosystem: The large community behind Linux distributions and the open-source software (OSS) ethos means that you can find a vast library of free tools and libraries specifically tailored to scientific and data-heavy workloads.
Server and Cloud Environments: Most servers, supercomputers, and cloud platforms run on some form of Linux. Understanding Linux helps you work seamlessly across development, testing, and production environments.
Powerful Command-Line Tools: Linux provides an extensive set of command-line tools such as grep, awk, sed, and more, which can rapidly process large text files and datasets directly from the terminal without requiring specialized software.
Customizability: Because Linux is open source, you have more freedom to customize the environment, install only the necessary components, and configure the system to suit your data workflows.
1.2 Common Linux Distributions
Distributions (often called distros) are different flavors of Linux that package the kernel, software repositories, and configuration tools. Some popular ones for data science include:

Ubuntu: A popular choice among beginners and experienced users. It has a large community, good support for recent software packages, and is widely used on desktops, servers, and cloud instances.
Debian: Known for its stability. Ubuntu is actually based on Debian, but Debian itself tends to update more conservatively, which some users prefer for production systems.
Fedora: Sponsored by Red Hat, Fedora is a cutting-edge distro that often includes the newest software. It can be a good environment if you want the latest libraries and frameworks.
CentOS / Rocky Linux / AlmaLinux: These are downstream rebuilds of Red Hat Enterprise Linux (RHEL). They’re typically used in production server environments where long-term stability is prioritized.
openSUSE: Another user-friendly distribution with strong enterprise support. It’s stable and has good community backing.
For a data science learner, Ubuntu (or a distribution derived from Ubuntu) is often the most straightforward choice due to its large community and user-friendly ecosystem. However, the concepts in this tutorial apply to almost any Linux distribution.

1.3 Installing and Getting Started
There are multiple ways to get started with Linux:

Dual Boot: Install Linux alongside your existing OS on your PC or laptop. This allows you to choose the operating system when you start up.
Virtual Machine (VM): Use software like VirtualBox or VMware to install Linux within your existing OS. This is a safer option for beginners and doesn’t require partitioning your hard drive.
Cloud/Server Instance: Provision a cloud-based Linux instance on providers like AWS, Google Cloud Platform (GCP), or Microsoft Azure. You can then connect via SSH. This is common for production data science workflows.
Windows Subsystem for Linux (WSL): On Windows 10 and 11, you can install a Linux subsystem that runs directly within Windows. It’s an excellent way to experiment with Linux terminal commands without leaving Windows.
1.4 The Terminal and Shell
The terminal (or console) is the command-line interface (CLI) that lets you interact with the system by typing commands. The shell is the program that processes these commands.

Common shells include:

Bash (Bourne-Again SHell): The default on many Linux systems.
Zsh: Known for its user-friendly features like auto-completion and theme-able prompt.
As a data scientist, you’ll likely spend a significant amount of time in the shell—navigating directories, running scripts, manipulating data files, and launching software.

Basic Command Structure
css
Copy
command [options
] [arguments
]
command: The executable or program to run (e.g., ls, cd, grep).
options: Often single-character flags (e.g., -l) or longer, more descriptive flags (e.g., --help).
arguments: Filenames, directories, or other data the command operates on.
1.5 Navigating Directories
When you first open a terminal, you’re typically placed in your home directory, something like /home/username. The root directory / is the base of the Linux filesystem. Important directories in Linux include:

/bin and /usr/bin: Common software binaries (executables).
/lib and /usr/lib: System libraries.
/etc: System configuration files.
/home: Home directories of all users.
/var: Log files and other variable data.
/tmp: Temporary files.
/opt: Optional or third-party software.
Key commands to know:

pwd: Print the current working directory path.
ls: List directory contents.
ls -l: Long listing format.
ls -a: Include hidden files.
ls -lh: Show file sizes in human-readable form.
cd <directory>: Change directory to <directory>.
cd ..: Go up one level.
cd ~: Go to your home directory.
cd -: Return to the previous directory.
1.6 Working with Files and Directories
Creating and Deleting
mkdir <directory>: Create a new directory.
rmdir <directory>: Remove an empty directory.
rm <file>: Remove a file.
rm -r <directory>: Remove a directory and its contents recursively.
touch <file>: Create a blank file (or update its timestamp if it exists).
Copying and Moving
cp <source> <destination>: Copy a file or directory.
cp -r <source_dir> <destination_dir>: Copy recursively.
mv <source> <destination>: Move or rename a file or directory.
Viewing Contents
cat <file>: Output file contents to the terminal.
less <file>: Paginate through file contents (use arrow keys or PgDn/PgUp, press q to exit).
head <file>: Show the first 10 lines (use -n for a different number).
tail <file>: Show the last 10 lines (same -n usage).
tail -f <file>: Follow the file in real-time—useful for monitoring log files.
1.7 Users, Groups, and Permissions
Linux is a multi-user system that distinguishes between regular users, the root user (administrator or superuser), and groups that can be assigned to one or more users.

Understanding File Permissions
Every file and directory in Linux has associated permissions split into three entities:

Owner (user): Usually the user who created the file.
Group: A collection of users who share certain privileges.
Others: Everyone else.
Permissions are typically shown as a string of 10 characters, e.g., -rwxr-xr--:

The first character indicates file type (- for a regular file, d for directory).
The next three characters are the owner (user) permissions (rwx).
The following three are the group permissions (r-x).
The last three are the permissions for others (r--).
Where:

r = read
w = write
x = execute
- = no permission
You can change file or directory permissions with chmod, ownership with chown, and group ownership with chgrp.

Switching Users and Privileges
sudo: Stands for “superuser do,” allows a permitted user to run commands as the superuser.
sudo <command>: Run <command> with elevated privileges.
sudo -i or sudo su: Start a root shell. Use carefully to avoid damaging your system.
su <username>: Switch user. You’ll need the password of the user you’re switching to.
1.8 The Linux Philosophy and the Power of the Command Line
Linux (and UNIX-like systems in general) follows a philosophy of creating small, modular programs that can be chained together to perform complex tasks. For example, if you want to see how many lines match a particular pattern in a file, you might do:

bash
Copy
grep "pattern" file.txt | wc -l
This command uses grep to search for "pattern" in file.txt and then pipes (|) the results to wc -l (word count in line mode) to get the line count. Understanding this principle—the pipeline—and mastering basic tools can greatly boost your efficiency in handling data.

Part II: Essential Command-Line Tools for Data Science
2.1 Searching and Pattern Matching
grep
grep stands for “global regular expression print.” It searches for lines matching a pattern in a file or stdin. Key options:

grep <pattern> <file>: Print lines in <file> that match <pattern>.
grep -i <pattern> <file>: Case-insensitive search.
grep -v <pattern> <file>: Invert match (show lines that do not match).
grep -r <pattern> <directory>: Recursively search through a directory.
Regular Expressions (regex)
Regex is a powerful way to specify search patterns. A few examples:

^: Matches the start of a line.
Example: grep "^Error" logfile matches lines starting with “Error.”
$: Matches the end of a line.
Example: grep "done$" script.sh matches lines ending with “done.”
.: Matches any single character.
*: Matches zero or more of the preceding element.
[]: Matches any one character in the set.
Example: grep "[0-9]" data.csv matches lines containing any digit.
Learning regex can be extremely useful for parsing logs, cleaning data, or quickly extracting relevant lines from large text-based datasets.

2.2 Filtering and Transforming Data
sed
sed (stream editor) is often used to search and replace text in a stream:

php
Copy
sed 's/<pattern>/<replacement>/g' <file>
s: Substitute.
g: Global replacement on each line.
You can also use sed in a pipeline without editing the file in-place:

bash
Copy
cat file.txt | sed 's/foo/bar/g'
To edit a file in-place, you can use:

arduino
Copy
sed -i 's/foo/bar/g' file.txt
Note: Use caution with sed -i, as it changes the file content permanently unless you specify a backup.

awk
awk is a scripting language designed for text processing. It can handle column-based operations in CSV or similar files. For example:

kotlin
Copy
awk -F, '{ print $2, $5
}' data.csv
-F, sets the field separator to a comma (typical for CSV files).
$2 refers to the second column, $5 refers to the fifth column.
You can also perform calculations:

kotlin
Copy
awk -F, '{ sum += $3
} END { print sum
}' data.csv
This aggregates the values in the third column of data.csv.

cut, sort, uniq
cut: Extract columns by position or delimiter.
cut -d',' -f2 data.csv extracts the second field in a comma-separated file.
sort: Sort lines in alphabetical or numerical order.
sort -n file.txt sorts numerically.
uniq: Removes or counts duplicate lines.
sort file.txt | uniq will remove duplicates.
sort file.txt | uniq -c will prefix each unique line with its count of occurrences.
These tools can be chained in a pipeline to perform powerful transformations on the fly. For instance:

bash
Copy
cut -d',' -f2 data.csv | sort | uniq -c | sort -nr
This command:

Extracts the second column,
Sorts the values,
Counts unique occurrences,
Sorts in numeric reverse order (most frequent first).
2.3 Viewing and Managing Processes
ps, top, and htop
ps aux: Show running processes with details such as CPU and memory usage, owner, and command.
top: Interactive process viewer. Press q to quit, shift + m to sort by memory, etc.
htop: An enhanced version of top (not always installed by default). It displays color-coded usage stats and allows easier navigation of processes.
kill and killall
Use kill to send signals to processes (commonly the TERM or KILL signals). For example:

bash
Copy
kill 1234
Terminates the process with PID (Process ID) 1234.

You can also kill processes by name with killall:

Copy
killall python
Closes all processes named python.

2.4 Package Management
Depending on your Linux distribution, you’ll have different package managers to install, update, and remove software. For Debian/Ubuntu-based systems, you’ll use apt (or the older apt-get). For Red Hat/Fedora-based systems, you’ll use dnf or yum.

Ubuntu/Debian:

sudo apt update: Updates the list of available packages.
sudo apt upgrade: Upgrades installed packages to the latest versions.
sudo apt install <package>: Installs the specified package.
sudo apt remove <package>: Removes a package but leaves configuration files.
sudo apt autoremove: Removes unused dependencies.
Fedora/Red Hat:

sudo dnf update: Updates the list of available packages.
sudo dnf upgrade: Upgrades installed packages.
sudo dnf install <package>: Installs the specified package.
sudo dnf remove <package>: Removes a package.
2.5 Text Editors
nano
A beginner-friendly editor. Basic usage:

nano <file>: Opens <file> in nano.
Use the arrow keys to navigate.
Keyboard shortcuts appear at the bottom (e.g., ^O to save, ^X to exit).
vim
A more advanced modal editor.

Command mode: Where you type commands (like :wq to save and quit).
Insert mode: Where you insert text.
Basic usage:

vim <file>: Open <file> in vim.
Press i to enter insert mode, type text.
Press Esc to return to command mode.
Type :w to save,
:q to quit.
Emacs
Another powerful text editor with extensive customization. Usage:

emacs <file>: Opens <file> in Emacs.
Has different modes, such as text mode, programming modes, etc.
Can be used in GUI or in the terminal.
Part III: Setting Up Your Data Science Environment
3.1 Installing Python and R
Python
Most modern Linux distributions come with Python installed. However, you often want a recent version of Python 3 and a virtual environment system.

Ubuntu: sudo apt install python3 python3-pip python3-venv
Fedora: sudo dnf install python3 python3-pip python3-virtualenv
Once installed, verify with:

css
Copy
python3 --version
pip3 --version
Virtual Environments
Create a Python virtual environment to isolate packages:

bash
Copy
python3 -m venv myenv
source myenv/bin/activate
Then install packages with pip install numpy pandas scipy scikit-learn.

Deactivate the environment with deactivate.

R
Install R from your distribution’s repository:

Ubuntu: sudo apt install r-base
Fedora: sudo dnf install R
You can also install packages from the R console:

r
Copy
install.packages("dplyr")
library(dplyr)
3.2 Conda for Package Management
Conda is a popular package manager and environment manager especially in data science circles (the default environment manager in Anaconda and Miniconda distributions).

Miniconda: A lightweight version of Anaconda with just conda and Python.
Anaconda: A full distribution containing Python, R, and over 100 data science packages.
To install Miniconda:

Download the installer (e.g., Miniconda3-latest-Linux-x86_64.sh).
bash Miniconda3-latest-Linux-x86_64.sh
Follow the prompts.
Once installed:

lua
Copy
conda create -n mydsenv python=3.9
conda activate mydsenv
conda install numpy pandas scikit-learn
3.3 JupyterLab and Jupyter Notebooks
Jupyter is critical for interactive data science work:

Copy
pip install jupyterlab
or with conda:

r
Copy
conda install -c conda-forge jupyterlab
Then:

Copy
jupyter lab
to launch the JupyterLab interface in your browser. By default, it runs on localhost: 8888 if you’re working locally.

3.4 Data Science Libraries and Tools
NumPy: Foundational library for numerical computations in Python.
Pandas: Essential for data manipulation and analysis.
Matplotlib / Seaborn: For data visualization.
scikit-learn: Machine learning algorithms (classification, regression, clustering, etc.).
TensorFlow / PyTorch: Deep learning frameworks.
Spark: Big data processing (install via packages or directly from the Spark website).
Part IV: Command-Line Data Wrangling
4.1 Using Shell Tools for Quick Data Inspection
The command line can handle large text-based datasets efficiently:

wc -l data.csv: Quickly see how many rows in data.csv.
head -n 100 data.csv > sample.csv: Extract a smaller subset for experimentation.
split -l 1000000 data.csv data_part_: Break a big file into 1-million-line chunks.
4.2 Example: CSV Cleaning Pipeline
Consider you have a CSV file with noisy data:

bash
Copy
cat data.csv | tr -d '\r' | grep -v "^#" | awk -F, '{ if ($3 != "") print $0
}' > clean_data.csv
Breaking it down:

tr -d '\r': Remove carriage return characters (helpful when dealing with Windows-origin files).
grep -v "^#": Remove lines starting with #, often used for comments.
awk -F, '{ if ($3 != "") print $0
}': Keep lines where the third column is non-empty.
> redirects the final result to clean_data.csv.
4.3 Combining Shell Tools and Python
While shell tools are great for quick manipulations, you might switch to Python for more complex tasks. You can seamlessly combine them:

bash
Copy
cut -d',' -f1,
5 data.csv | python process_data.py
Here, cut extracts columns 1 and 5 from data.csv, then pipes them into a Python script process_data.py.

Part V: Shell Scripting and Automation
5.1 Writing Your First Shell Script
A shell script is just a text file containing commands that can be executed sequentially. Start with the shebang line to indicate which shell to use:

bash
Copy
#!/usr/bin/env bash

# A simple shell script
echo "Hello, Data Science!"
Make it executable:

bash
Copy
chmod +x myscript.sh
./myscript.sh
5.2 Variables and Arguments
Variables
In bash, assigning a variable looks like:

bash
Copy
#!/usr/bin/env bash

NAME="Alice"
echo "Hello, $NAME!"
Note that there’s no space before or after the = sign. Use $NAME to reference the variable.

Arguments
Inside a shell script, $1, $2, $3, etc., refer to positional arguments passed to the script:

bash
Copy
#!/usr/bin/env bash

echo "The first argument is $1"
echo "The second argument is $2"
When running:

bash
Copy
./myscript.sh argument1 argument2
Outputs:

csharp
Copy
The first argument is argument1
The second argument is argument2
5.3 Conditionals and Loops
If Statements
bash
Copy
if [ $1 -gt 100
]; then
    echo "Argument is greater than 100"
else
    echo "Argument is less than or equal to 100"
fi
-gt stands for “greater than,” and similar numeric operators exist (-lt, -eq, -ne, etc.). For string comparison, use = or !=.

For Loops
bash
Copy
for i in {1..5
}; do
    echo "Iteration $i"
done
You can also loop through file patterns:

bash
Copy
for file in *.txt; do
    echo "Processing $file"
done
5.4 Scheduling Jobs with cron
cron is a system daemon used to schedule commands or scripts to run periodically.

crontab
Use crontab -e to edit your cron jobs. A crontab entry has the format:

sql
Copy
*  *  *  *  *   /path/to/command
|  |  |  |  |
|  |  |  |  └─ Day of week (0-7)
|  |  |  └──── Month (1-12)
|  |  └─────── Day of month (1-31)
|  └────────── Hour (0-23)
└───────────── Minute (0-59)
Example:

javascript
Copy
0 2 * * * /home/user/scripts/backup.sh
Runs backup.sh at 2: 00 AM every day.

Part VI: Collaboration and Version Control
6.1 Git Basics
git is the most widely used version control system:

git init: Initialize a repository in the current directory.
git clone <repo_url>: Clone a remote repository to your local machine.
git add <file>: Stage changes for commit.
git commit -m "Message": Commit staged changes with a descriptive message.
git push: Upload your local commits to a remote repository (e.g., GitHub or GitLab).
git pull: Fetch and merge changes from the remote repository.
6.2 Branching Workflow
csharp
Copy
git branch new-feature
git checkout new-feature
# Make changes, commits
git checkout main
git merge new-feature
In data science, branching is useful to experiment with new analyses or models without affecting the production or main code base.

6.3 GitHub, GitLab, and Bitbucket
GitHub: The most popular host for open-source projects. Great for collaboration, pull requests, and code reviews.
GitLab: Offers integrated DevOps features (CI/CD pipelines).
Bitbucket: Popular with teams that use Atlassian products (Jira, Confluence).
Part VII: Working with Remote Servers
7.1 SSH and Remote Access
ssh username@server_address: Securely connects to a remote Linux server.
scp file username@server_address:~/dest: Securely copy file to the remote server (destination path ~/dest).
rsync -avz local_dir username@server_address:~/dest_dir: Efficiently synchronize directories between local and remote.
7.2 Tunneling and Port Forwarding
Sometimes you need to securely forward a port from the server to your local machine, for instance to access a Jupyter notebook running on a remote server:

ruby
Copy
ssh -L 8888:localhost: 8888 username@server_address
Then open http: //localhost:8888 in your local browser to access the remote Jupyter notebook.

Part VIII: Introduction to High-Performance Computing (HPC)
8.1 Why HPC for Data Science?
When datasets grow huge or models become very computationally intensive (e.g., deep learning), you may need:

More CPU/GPU resources.
Faster interconnects (InfiniBand).
Parallel computing frameworks (MPI, Spark, Dask).
Linux is the default environment in HPC clusters because of its stability and flexibility.

8.2 Job Schedulers
Common HPC clusters use a job scheduler like:

SLURM (Simple Linux Utility for Resource Management).
PBS (Portable Batch System).
LSF (IBM Spectrum LSF).
You typically submit jobs via a job script. For example, with SLURM:

bash
Copy
#!/usr/bin/env bash
#SBATCH --job-name=myjob
#SBATCH --ntasks=4
#SBATCH --time=01: 00: 00
#SBATCH --mem=4G

module load python/3.9
srun python my_script.py
Submit this job with sbatch myjob.sh.

8.3 Parallel Processing with Python
Libraries such as multiprocessing and frameworks like Dask allow you to spread tasks across multiple cores or nodes:

python
Copy
from dask import delayed
import dask.multiprocessing

def square(x):
    return x * x

results = []
for i in range(1000):
    results.append(delayed(square)(i))

total = delayed(sum)(results)
print(total.compute())
Dask automatically parallelizes the workload across available cores.

Part IX: Containerization for Data Science
9.1 Docker Basics
Docker packages your environment and application into containers, ensuring consistency across different machines.

docker build -t mydatascience .
docker run -it mydatascience
Dockerfile example:

Dockerfile
Copy
FROM python: 3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .

CMD [
    "python",
    "analysis.py"
]
9.2 Singularity/Apptainer in HPC
Many HPC centers use Singularity (now Apptainer) instead of Docker because of security features. You can build a container on your local machine with Docker, then convert it to Singularity for HPC deployment.

Part X: Security and Best Practices
10.1 System Updates and Security Patches
Keep your system up to date:

Ubuntu: sudo apt update && sudo apt upgrade
Fedora: sudo dnf upgrade
Regularly review security advisories and patches.
10.2 Firewall Configuration
Most distros include iptables or firewalld:

Ubuntu uses ufw (Uncomplicated Firewall) as a user-friendly interface.
sudo ufw status
sudo ufw enable
sudo ufw allow 22 (allows SSH).
sudo ufw allow 8888 (for Jupyter).
10.3 Using SSH Keys
Instead of passwords, generate SSH key pairs for secure login:

typescript
Copy
ssh-keygen -t rsa -b 4096
cat ~/.ssh/id_rsa.pub | ssh username@server_address "mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys"
Now you can ssh username@server_address without a password (assuming correct permissions).

10.4 Environment and Package Isolation
Use virtual environments or conda environments to avoid dependency conflicts.
Leverage containers (Docker, Singularity) for consistent, reproducible environments.
Part XI: Performance Monitoring and Optimization
11.1 Monitoring Tools
nvidia-smi: Monitors GPU usage (NVIDIA GPUs).
vmstat, iostat, sar: Collect and report system activity, CPU, memory, IO usage.
iotop: Monitors disk I/O in real time.
11.2 System Profiling
time command: Basic measurement of how long a command takes.
css
Copy
time python my_script.py
cProfile (Python): Built-in profiler.
python
Copy
import cProfile
cProfile.run('my_function()')
11.3 Memory Tuning
Use swapoff -a on HPC systems to see if you’re relying on swap (not always recommended in production).
Increase memory usage? Plan your scripts so they read data in chunks or use out-of-core libraries such as dask.
Part XII: Advanced Topics
12.1 Building Software from Source
Sometimes you need the latest or a custom version of a library:

Install dependencies: Often the -dev versions of libraries.
Download source (e.g., git clone <repo_url>).
Configure, build, install:
go
Copy
./configure
make
sudo make install
12.2 Systemd Services
You can create system services (like for a web application, data pipeline, or JupyterHub) using systemd:

bash
Copy
sudo nano /etc/systemd/system/myapp.service
Example unit file:

makefile
Copy
[Unit
]
Description=My Data Science App

[Service
]
ExecStart=/usr/bin/python /home/user/myapp.py
Restart=always
User=user

[Install
]
WantedBy=multi-user.target
Then:

bash
Copy
sudo systemctl daemon-reload
sudo systemctl enable myapp
sudo systemctl start myapp
12.3 Remote Development with VS Code
VS Code has remote development extensions that allow you to code directly on a remote Linux server:

Remote SSH: Use SSH to open a remote folder on the server and work as if it’s local.
12.4 Data Science Pipelines on Linux
Airflow: Workflow orchestration, good for data engineering.
Luigi: Simplifies complex pipeline management with tasks and dependencies.
Prefect: Modern approach to pipeline orchestration.
These tools typically run on Linux-based servers and leverage Python for pipeline definitions.

Part XIII: Putting It All Together
13.1 A Typical Data Science Workflow on Linux
Environment Setup:

Create a conda environment with Python libraries.
Possibly containerize it with Docker or Singularity for reproducibility.
Data Ingestion:

Use scp or rsync to transfer raw data to your Linux machine.
Quick checks with head, wc -l, or grep.
Data Cleaning:

Filter out unwanted rows with awk and sed.
Alternatively, load into Python (pandas) for more complex transformations.
Exploratory Analysis:

Use Jupyter notebooks (local or remote with SSH tunneling).
Visualize with matplotlib or seaborn.
Modeling:

Use scikit-learn, TensorFlow, or PyTorch.
If the dataset is huge, switch to HPC or distributed computing with Spark/Dask.
Deployment:

Package scripts into Docker containers.
Deploy in production or schedule with cron, systemd, or an orchestration tool like Kubernetes.
Monitoring and Optimization:

Monitor system load with top, GPU usage with nvidia-smi.
Profile scripts, adjust environment, and possibly scale to HPC.
13.2 Tips for Success
Learn the Command Line: Mastering Linux commands (pipes, redirection, text processing) will save you huge amounts of time in data wrangling tasks.
Keep Things Automated: Shell scripts, cron jobs, and pipeline managers help you avoid repetitive manual tasks.
Version Control Everything: Not just code, but also your config files or environment definitions (via Dockerfiles, environment.yml, etc.).
Security & Reliability: Keep your system patched, use SSH keys, manage firewall rules, and test your environment thoroughly.
Part XIV: Resources and Further Reading
Official Linux Documentation:

Linux Documentation Project (though not updated frequently, still a good historical resource).
Distro-specific docs (e.g., Ubuntu Docs, Fedora Docs).
Books:

The Linux Command Line by William Shotts.
UNIX and Linux System Administration Handbook by Evi Nemeth et al.
Online Courses:

edX Linux Courses
Coursera Data Science Specializations
Tools:

Docker Documentation
Singularity / Apptainer Docs
Dask Documentation
Airflow Documentation
HPC:

SLURM Docs
OpenMPI
MPI4Py
